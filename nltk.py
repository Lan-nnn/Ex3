# -*- coding: utf-8 -*-
"""nltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tl123bP6nT8cBUNWu5fSWfkeO9eEepm5
"""

import nltk
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from collections import Counter

# 下载必要的数据和资源
nltk.download('gutenberg')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.corpus import gutenberg
from nltk.stem import WordNetLemmatizer

# 读取Moby Dick文件
moby_dick_text = gutenberg.raw('melville-moby_dick.txt')

# 1. 分词
tokens = nltk.word_tokenize(moby_dick_text)

# 2. 停用词过滤
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

# 3. 词性标注
pos_tags = nltk.pos_tag(filtered_tokens)

# 4. 词性频率统计
pos_counts = Counter(tag for word, tag in pos_tags)
common_pos = pos_counts.most_common(5)

# 5. 词形还原
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word, tag in pos_tags[:20]]

# 6. 绘制频率分布图
freq_dist = FreqDist(tag for word, tag in pos_tags)
freq_dist.plot(30, cumulative=False)

# 打印结果
print("5 most common parts of speech with their frequencies:")
for pos, freq in common_pos:
    print(f"{pos}: {freq}")

print("\nTop 20 tokens after lemmatization:")
for token in lemmatized_tokens:
    print(token)